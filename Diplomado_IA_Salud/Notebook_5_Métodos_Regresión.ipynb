{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<table>\n","    <tr>\n","        <td><img src=\"https://www.acofi.edu.co/eiei2016/wp-content/uploads/2016/09/Logo-Universidad-EIA.jpg\" width=\"250\"/></td>\n","        <td>&nbsp;</td>\n","        <td>\n","        <td><img src=\"https://raw.githubusercontent.com/Fabian830348/Bases_Datos/refs/heads/master/Logo_EICT_horizontal_ESPANOL%20(1).png\" width=\"300\"/></td>\n","        <td>&nbsp;</td>\n","        <td>\n","            <h1 style=\"font-size:200%;color:blue;text-align:center\">    <FONT COLOR=\"blue\"> Regresión </p> Métodos de Regresión </FONT>         </h1></td>         \n","        <td>\n","            <tp><p style=\"font-size:99%;text-align:center\">Machine Learning </p></tp>\n","            <tp><p style=\"font-size:115%;text-align:center\">A. Supervisado</p></tp>\n","            <tp><p style=\"font-size:115%;text-align:center\">Prof. Fabián Sánchez</p></tp>\n","        </td>\n","    </tr>\n","</table>"],"metadata":{"id":"ESsuqTEZrNcp"}},{"cell_type":"markdown","source":["# <FONT SIZE=5 COLOR=\"purple\"> 1. Concepto de Regresión </FONT>\n","\n","**Modelo de regresión:** es el modelo en el que se busca establecer la relación entre un cierto número de características y una variable objetivo continua. Es decir, la relación entre la variable dependiente $y$ y las variables predictoras $x_1, x_2 , \\dots , x_n$.\n","\n","En los modelos de regresión no se predice una clase, sino un valor continuo.\n","\n","Anteriormente habíamos examinado los diferentes métodos de clasificación, conocidos como *classifier*. En esta sección revisaremos las instrucciónes para generar las regresiones (regressor).\n","\n","https://keepcoding.io/blog/como-funciona-el-k-nn-en-regresion/\n","\n","https://fhernanb.github.io/libro_mod_pred/arb-de-regre.html\n","\n","https://www.cienciadedatos.net/documentos/py07_arboles_decision_python.html\n","\n","https://towardsdatascience.com/random-forest-regression-5f605132d19d\n","\n","## <FONT SIZE=4 COLOR=\"blue\"> 1.1 k-Nearest Neighbors (kNN) para regresión </FONT>\n","\n","En lugar de clasificar una muestra, el kNN para regresión predice un valor numérico, como sigue:\n","\n","1. **Identificación de los vecinos más cercanos**: Dado un nuevo punto de datos, el algoritmo busca los **k** puntos más cercanos en el espacio de características (según una medida de distancia como la euclidiana).\n","   \n","2. **Cálculo del valor predicho**: El valor de salida para el punto nuevo se calcula promediando los valores de las etiquetas de los **k** vecinos más cercanos. Es decir:\n","$$   \\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} y_i $$\n","   \n","donde $ y_i $ son las etiquetas de los k-vecinos más cercanos.\n","\n","3. **Suavizado del resultado**: Como resultado, el valor de salida es continuo, lo que convierte al kNN en una técnica válida para regresión.\n","\n","## <FONT SIZE=4 COLOR=\"blue\"> 1.2 Árboles de decisión para regresión </FONT>\n","\n","En los árboles de decisión para regresión, el procedimiento también cambia respecto al de clasificación:\n","\n","1. **División del conjunto de datos**: El árbol se construye dividiendo el conjunto de datos en ramas con base en los valores de las características. Sin embargo, en lugar de buscar divisiones que maximicen la ganancia de información o el índice de Gini (usado en clasificación), en regresión se busca minimizar la **varianza** o el **error cuadrático medio** (MSE, por sus siglas en inglés).\n","\n","2. **Predicción**: Una vez que el árbol está entrenado, la predicción para un nuevo punto de datos se realiza navegando por el árbol desde la raíz hasta una hoja. El valor en la hoja no es una clase, sino un valor promedio de las etiquetas de los datos que caen en esa hoja.\n","\n","3. **Criterio de división**: Para cada nodo, el árbol selecciona la característica y el punto de división que minimicen la suma del error cuadrático medio en los subconjuntos hijos.\n","\n","El modelo final hace una predicción basada en los valores promedio de los datos en los nodos terminales, obteniendo así una predicción numérica.\n","\n","### Comparación:\n","- En **kNN para regresión**, la predicción es un promedio de los vecinos cercanos.\n","- En **árboles de decisión para regresión**, la predicción es el valor promedio de los datos en las hojas del árbol.\n","\n","## <FONT SIZE=4 COLOR=\"blue\"> 1.3 Error Cuadrático Medio </FONT>\n","\n","El **Error Cuadrático Medio** (MSE, por sus siglas en inglés de *Mean Squared Error*) es una medida de la calidad de un modelo de regresión que cuantifica la diferencia promedio entre los valores predichos por el modelo y los valores reales de las observaciones.\n","\n","El MSE es especialmente útil para medir el rendimiento en problemas de regresión, ya que da una idea de cuán lejos están las predicciones del modelo respecto a los valores reales en promedio.\n","\n","Este valor se define como el promedio de los cuadrados de las diferencias entre los valores reales $y_i $ y los valores predichos $\\hat{y}_i$:\n","\n","$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n","\n","donde:\n","\n","- $n$ es el número total de ejemplos en el conjunto de datos.\n","\n","- $y_i$ es el valor real para la observación $i$.\n","\n","- $\\hat{y}_i$ es el valor predicho por el modelo para la observación $i$.\n","\n","**Explicación de la fórmula**\n","\n","a. **Diferencia**: Para cada punto de datos, calculamos la diferencia entre el valor real $y_i$ y el valor predicho $\\hat{y}_i$.\n","   \n","b. **Cuadrado de la diferencia**: Elevamos al cuadrado cada una de estas diferencias. Esto garantiza que los errores positivos y negativos no se cancelen entre sí y penaliza más los errores grandes.\n","\n","c. **Promedio**: Sumamos los cuadrados de las diferencias y dividimos por el número total de puntos de datos para obtener el promedio de esos errores cuadráticos.\n","\n","El MSE siempre es positivo y un valor más bajo de MSE indica un mejor ajuste del modelo a los datos.\n"],"metadata":{"id":"fVn3HrgY26X-"}},{"cell_type":"markdown","source":["<FONT SIZE=5 COLOR=\"blue\"> Librería de trabajo </FONT>"],"metadata":{"id":"DwKLGLePOqOk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhXkA_Cofjng"},"outputs":[],"source":["# Para gráficos y data.frames\n","import pandas as pd\n","import numpy  as np\n","\n","# librerías para graficar\n","import plotly.express     as px\n","import matplotlib.pyplot  as plt\n","import seaborn            as sns\n","\n","\n","# Dividir los datos entrenamiento y prueba\n","from sklearn.model_selection     import train_test_split\n","from sklearn.feature_selection   import RFE\n","\n","# Para preprocesamiento: escalador\n","from sklearn.preprocessing       import StandardScaler\n","\n","# Modelo de regresión\n","from sklearn.linear_model        import LinearRegression\n","from sklearn.neighbors           import KNeighborsRegressor\n","from sklearn.tree                import DecisionTreeRegressor\n","\n","from sklearn.ensemble            import RandomForestRegressor\n","from sklearn.svm                 import SVR\n","# Métricas\n","from sklearn                     import metrics\n","from sklearn.metrics             import mean_squared_error, r2_score"]},{"cell_type":"markdown","source":["<FONT SIZE=5 COLOR=\"blue\"> Idea intuitiva gráfica </FONT>"],"metadata":{"id":"wXdvMekMOuh0"}},{"cell_type":"code","source":["# gráfico para knn\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","# Generamos datos de ejemplo\n","np.random.seed(42)\n","X = np.sort(5 * np.random.rand(80, 1), axis=0)\n","y = np.sin(X).ravel() + np.random.randn(80) * 0.2\n","\n","# Creamos un modelo de kNN para regresión\n","knn_regressor = KNeighborsRegressor(n_neighbors=5)\n","knn_regressor.fit(X, y)\n","\n","# Predicciones\n","X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n","y_knn_pred = knn_regressor.predict(X_test)\n","\n","# Gráfica\n","plt.figure(figsize=(10, 6))\n","plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"Datos reales\")\n","plt.plot(X_test, y_knn_pred, color=\"green\", label=\"Predicción del kNN (k=5)\", linewidth=2)\n","plt.xlabel(\"X\")\n","plt.ylabel(\"Y\")\n","plt.title(\"k-Nearest Neighbors para regresión\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.show()\n"],"metadata":{"id":"XvAGQNW19HvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gráfico para árboles de decisión\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from  sklearn.tree import DecisionTreeRegressor\n","\n","# Generamos datos de ejemplo\n","np.random.seed(42)\n","X = np.sort(5 * np.random.rand(80, 1), axis=0)\n","y = np.sin(X).ravel() + np.random.randn(80) * 0.2\n","\n","# Ajustamos un árbol de decisión para regresión\n","tree_regressor = DecisionTreeRegressor(max_depth=5)\n","tree_regressor.fit(X, y)\n","\n","# Predicciones\n","X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n","y_pred = tree_regressor.predict(X_test)\n","\n","# Gráfica\n","plt.figure(figsize=(10, 6))\n","plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"Datos reales\")\n","plt.plot(X_test, y_pred, color=\"cornflowerblue\", label=\"Predicción del árbol de decisión\", linewidth=2)\n","plt.xlabel(\"X\")\n","plt.ylabel(\"Y\")\n","plt.title(\"Árbol de decisión para regresión\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.show()"],"metadata":{"id":"YITZ7f7f9LxX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <FONT SIZE=5 COLOR=\"purple\"> 2. Ejemplo 1 de Regresión (calorias) </FONT>\n","\n","Vamos a considerar los datos de kaggle\n","\n","https://www.kaggle.com/code/samtam22/exercise-and-calorie-burning-data-analysis\n","\n","que contienen las siguientes variables.\n","\n","- **User_Id** : Identificador único para cada individuo del conjunto de datos.\n","\n","  - Sirve para vincular diferentes registros del mismo usuario (por ejemplo, de dos datasets: uno de ejercicios y otro de calorías).\n","\n","- **Gender**: Género del usuario.  En los datos suelen estar codificados como: male = 0, female = 1, o viceversa\n","\n","- **Age** : Edad del usuario en años.\n","\n","- **Height**: Estatura del usuario, normalmente expresada en centímetros (cm)\n","\n","- **Weight** : Peso del usuario en kilogramos (kg).\n","\n","- **Duration**:  Duración del ejercicio o actividad física, generalmente expresada en minutos\n","\n","- **Heart_Rate**: Frecuencia cardíaca promedio durante el ejercicio, medida en pulsos por minuto (bpm)\n","\n","- **Body_Temp**: Temperatura corporal promedio durante la actividad, en grados Celsius (°C)\n","\n","- **Calories**: Calorías quemadas estimadas durante el ejercicio.\n","\n","Es la variable objetivo en modelos que predicen gasto energético en base a los factores anteriores"],"metadata":{"id":"QM6Vj79mL73m"}},{"cell_type":"code","source":["calorias = pd.read_csv(\"https://raw.githubusercontent.com/Fabian830348/Bases_Datos/refs/heads/master/calories.csv\")\n","calorias"],"metadata":{"id":"3oVUaEOhABi8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ejercicio = pd.read_csv(\"https://raw.githubusercontent.com/Fabian830348/Bases_Datos/refs/heads/master/exercise.csv\")\n","ejercicio"],"metadata":{"id":"HFl5jXQpAZMU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos a fusionar los datos"],"metadata":{"id":"dtUelf_MAj2N"}},{"cell_type":"code","source":["# pegar los dos dataframes\n","datos = pd.concat([calorias, ejercicio], axis=1)\n","datos"],"metadata":{"id":"TSZPbM_WAmDA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos a eliminar la variable User_ID"],"metadata":{"id":"3mztXG7cBDgn"}},{"cell_type":"code","source":["# eliminar User_ID\n","datos = datos.drop('User_ID', axis=1)\n","datos"],"metadata":{"id":"sywpsOT3BL85"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hacemos una pequeña exploración de los datos"],"metadata":{"id":"9qXnz83wBQbc"}},{"cell_type":"code","source":["# cabeza de los datos\n","datos.head()"],"metadata":{"id":"6Fha4_GgBUBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tamaño de los datos\n","datos.shape"],"metadata":{"id":"ANt30Pn1D2PC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# variables de los datos\n","datos.columns"],"metadata":{"id":"DwCunDzoDo-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# información de los datos\n","datos.info()"],"metadata":{"id":"P02GAZIxDs4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# estadística de los datos\n","datos.describe()"],"metadata":{"id":"qKD8mpcrD0rC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mapa de correlaciones de las variables numéricas con seaborn\n","num_cols = datos.select_dtypes(include=[\"number\"])\n","corr = num_cols.corr()\n","# Mapa de calor\n","plt.figure(figsize=(10,8))\n","sns.heatmap(corr, annot=True,\n","            xticklabels=num_cols.columns,\n","            yticklabels=num_cols.columns,\n","            cmap=\"coolwarm\", center=0)\n","plt.show()\n"],"metadata":{"id":"7dNreSMjEKaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gender convertir a 1 y 0\n","datos.Gender.replace({\"male\":1, \"female\":0}, inplace= True)\n"],"metadata":{"id":"dNoWm-WhFFeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datos.head()"],"metadata":{"id":"R78wzi29FqdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convertir gender a categórica\n","datos.Gender = datos.Gender.astype(\"category\")"],"metadata":{"id":"z8bPn_EGFyrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datos.head()"],"metadata":{"id":"dBjf60NoF8Sl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# seleccionar las variables para los modelos\n","X = datos.drop(\"Calories\", axis=1)\n","y = datos[\"Calories\"]\n","\n","# seleccionar los conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size=0.2,\n","                                                    random_state=42)\n","\n","# Escalar los datos\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"],"metadata":{"id":"vm3YhNcmF-T_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"magenta\"> 2.1 Modelo con KNN </FONT>"],"metadata":{"id":"HEEvIPqiHDSn"}},{"cell_type":"code","source":["# librería\n","from sklearn.neighbors import KNeighborsRegressor\n","# modelo\n","KNN = KNeighborsRegressor(n_neighbors=5)\n","# entrenamiento\n","KNN.fit(X_train_scaled, y_train)\n","# predicciones\n","y_pred_knn = KNN.predict(X_test_scaled)\n","# métricas\n","print(\"R2\",r2_score(y_test, y_pred_knn))                             # coeficiente de determinación\n","print('MSE', mean_squared_error(y_test, y_pred_knn))                 # error cuadrático medio\n","print('RMSE', np.sqrt(mean_squared_error(y_test,y_pred_knn)))        # raíz error cuadrático medio"],"metadata":{"id":"DdCgg5bQHH8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calcular métricas básicas\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred_knn))\n","# estadísticas de la variable objetivo\n","y_mean = datos[\"Calories\"].mean()\n","y_std = datos[\"Calories\"].std()\n","y_min = datos[\"Calories\"].min()\n","y_max = datos[\"Calories\"].max()\n","y_range = y_max - y_min\n","\n","# comparaciones relativas\n","rmse_vs_mean = rmse / y_mean\n","rmse_vs_std = rmse / y_std\n","rmse_vs_range = rmse / y_range\n","\n","# imprimir resultados\n","print(f\"RMSE: {rmse:.2f}\")\n","print(f\"RMSE vs media: {rmse_vs_mean:.2%}\")\n","print(f\"RMSE vs desviación estándar: {rmse_vs_std:.2%}\")\n","print(f\"RMSE vs rango: {rmse_vs_range:.2%}\")"],"metadata":{"id":"gyYjqWyWIF7i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"magenta\"> 2.2 Modelo con Regresión Lineal </FONT>"],"metadata":{"id":"I-ePBX-XHI4F"}},{"cell_type":"code","source":["# modelo de regresión lineal\n","modelo = LinearRegression()\n","# entrenamiento\n","modelo.fit(X_train_scaled,y_train)\n","# predicción\n","y_pred_linear= modelo.predict(X_test_scaled)\n","# evaluación del modelo\n","print(\"R2\",r2_score(y_test, y_pred_linear))                             # coeficiente de determinación\n","print('MSE', mean_squared_error(y_test, y_pred_linear))                 # error cuadrático medio\n","print('RMSE', np.sqrt(mean_squared_error(y_test,y_pred_linear)))        # raíz error cuadrático medio"],"metadata":{"id":"nMWg-cI5HOF7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calcular métricas básicas\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n","# estadísticas de la variable objetivo\n","y_mean = datos[\"Calories\"].mean()\n","y_std = datos[\"Calories\"].std()\n","y_min = datos[\"Calories\"].min()\n","y_max = datos[\"Calories\"].max()\n","y_range = y_max - y_min\n","\n","# comparaciones relativas\n","rmse_vs_mean = rmse / y_mean\n","rmse_vs_std = rmse / y_std\n","rmse_vs_range = rmse / y_range\n","\n","# imprimir resultados\n","print(f\"RMSE: {rmse:.2f}\")\n","print(f\"RMSE vs media: {rmse_vs_mean:.2%}\")\n","print(f\"RMSE vs desviación estándar: {rmse_vs_std:.2%}\")\n","print(f\"RMSE vs rango: {rmse_vs_range:.2%}\")"],"metadata":{"id":"pqtBI1APJqsq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"magenta\"> 2.3 Modelo con árboles de Decisión </FONT>"],"metadata":{"id":"d3kEuiO0JRv_"}},{"cell_type":"markdown","source":["Vamos a trabajar con los datos sin escalar"],"metadata":{"id":"-kVaK8bXPWNn"}},{"cell_type":"code","source":["# modelo\n","Tree_reg = DecisionTreeRegressor(random_state= 0)\n","# entrenamiento\n","Tree_reg.fit(X_train, y_train)\n","# predicción\n","y_pred_tree = Tree_reg.predict(X_test)\n","# evaluación del modelo\n","print(\"R2\",r2_score(y_test, y_pred_tree))                             # coeficiente de determinación\n","print('MSE', mean_squared_error(y_test, y_pred_tree))                 # error cuadrático medio\n","print('RMSE', np.sqrt(mean_squared_error(y_test,y_pred_tree)))        # raíz error cuadrático medio"],"metadata":{"id":"HvIEfuubJVse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calcular métricas básicas\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n","# estadísticas de la variable objetivo\n","y_mean = datos[\"Calories\"].mean()\n","y_std = datos[\"Calories\"].std()\n","y_min = datos[\"Calories\"].min()\n","y_max = datos[\"Calories\"].max()\n","y_range = y_max - y_min\n","\n","# comparaciones relativas\n","rmse_vs_mean = rmse / y_mean\n","rmse_vs_std = rmse / y_std\n","rmse_vs_range = rmse / y_range\n","\n","# imprimir resultados\n","print(f\"RMSE: {rmse:.2f}\")\n","print(f\"RMSE vs media: {rmse_vs_mean:.2%}\")\n","print(f\"RMSE vs desviación estándar: {rmse_vs_std:.2%}\")\n","print(f\"RMSE vs rango: {rmse_vs_range:.2%}\")"],"metadata":{"id":"L9X9U0rKJfJC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Profundidad del árbol: {Tree_reg.get_depth()}\")\n","print(f\"Número de nodos terminales: {Tree_reg.get_n_leaves()}\")"],"metadata":{"id":"05GfOZglKeYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from graphviz                 import Source\n","from sklearn.tree             import export_graphviz\n","Tree_reg2 = DecisionTreeRegressor(max_depth= 4,\n","                                  random_state= 0)\n","Tree_reg2.fit(X_train, y_train)\n","# Generamos el árbol\n","dot_data = export_graphviz(Tree_reg2,                                 # modelo\n","                           feature_names = X_train.columns,            # columnas de entrenamiento\n","                           filled=True)                                # colores del árbol (relleno)\n","Source(dot_data, format=\"png\")"],"metadata":{"id":"9ANT1qYCJ9Ca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# variables más importantes en árboles de decisión\n","importances = Tree_reg.feature_importances_\n","importances"],"metadata":{"id":"dkM4dTrIK7G6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Importancia de los predictores en el modelo\")\n","print(\"-------------------------------------------\")\n","importancia_predictores = pd.DataFrame(\n","    {'predictor': X_train.columns.tolist(),\n","     'importancia': Tree_reg.feature_importances_}\n",")\n","importancia_predictores.sort_values('importancia', ascending=False)"],"metadata":{"id":"WDgSH44lLELP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <FONT SIZE=5 COLOR=\"purple\"> 3. Ejemplo 2 Regresión (ventas) </FONT>\n","\n","Vamos a considerar los siguientes datos relacionados con la inversión en publicidad en diferentes medios y las ventas.\n","\n","El objetivo es hacer un modelo de regresión lineal múltiple para predecir las ventas en función de las variables de pauta publicitaria en diferentes medios.\n","\n","Se describen las variables independientes: TV, Radio Newpaper y la variable dependiente Sales.\n","\n","Valor de etiqueta o variable objetivo dependiente(ventas): que significa el volumen de ventas del producto correspondiente\n","\n","Las variables independientes: (TV, Radio, Periódico, WEB):\n","\n","TV: para un solo producto en un mercado determinado, el costo de la publicidad en TV (en miles) Radio: costos de publicidad invertidos en medios de difusión Periódico: costos publicitarios para medios periodísticos."],"metadata":{"id":"iXHfdo2pLdxw"}},{"cell_type":"markdown","source":["## <FONT SIZE=4 COLOR=\"green\"> 3.1 Carga y exploración de los datos</FONT>"],"metadata":{"id":"1eTXFoKXL-HA"}},{"cell_type":"code","source":["ventas = pd.read_csv(\"https://raw.githubusercontent.com/Fabian830348/Bases_Datos/refs/heads/master/Ventas.csv\", sep = \";\" , decimal = \",\" )\n","ventas"],"metadata":{"id":"raqVgfHsL8bS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# eliminar la primera columna\n","ventas = ventas.drop(ventas.columns[0],\n","                     axis=1)"],"metadata":{"id":"6w9HlmUJMAe-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ventas.head()"],"metadata":{"id":"EtPg9eo3Zi4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corr = ventas.corr()\n","corr"],"metadata":{"id":"pl19zuh3MBgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"l714jdHfPkdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# matriz de correlación\n","sns.heatmap(corr, annot = True,\n","            yticklabels=corr.columns,\n","            xticklabels=corr.columns)"],"metadata":{"id":"4Xa5bJk3ME0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Primero, seleccionamos las variables predictoras y la variable objetivo"],"metadata":{"id":"IWygnDGFMHsG"}},{"cell_type":"code","source":["# variable objetivo\n","y = ventas['Sales']\n","# variables predictoras\n","X = ventas.drop('Sales', axis=1)"],"metadata":{"id":"qWezwRHbMHBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# conjunto de entrenamiento y prueba\n","X_train,X_test,y_train,y_test=train_test_split(X,\n","                                               y,\n","                                               train_size=0.70,\n","                                               random_state=123)"],"metadata":{"id":"ve1xCc_BML_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler = StandardScaler()\n","X_train_s = scaler.fit_transform(X_train)\n","X_test_s = scaler.transform(X_test)"],"metadata":{"id":"lM99c8U2MODG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <FONT SIZE=4 COLOR=\"green\"> 3.2 Modelo con KNN </FONT>"],"metadata":{"id":"A1KcHiKgMPZe"}},{"cell_type":"code","source":["# librería\n","from sklearn.neighbors import KNeighborsRegressor\n","# modelo\n","KNN = KNeighborsRegressor(n_neighbors=5)\n","# entrenamiento\n","KNN.fit(X_train_s, y_train)\n","# predicciones\n","y_pred = KNN.predict(X_test_s)\n","# métricas\n","print(\"R2\",r2_score(y_test, y_pred))                             # coeficiente de determinación\n","print('MSE', mean_squared_error(y_test, y_pred))                 # error cuadrático medio\n","print('RMSE', np.sqrt(mean_squared_error(y_test,y_pred)))        # raíz error cuadrático medio"],"metadata":{"id":"v9IpZ83zMRLO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <FONT SIZE=4 COLOR=\"green\"> 3.3 Modelo con Árboles de Decisión </FONT>\n"],"metadata":{"id":"Nel8qiGQMVq8"}},{"cell_type":"code","source":["# modelo\n","Tree_reg = DecisionTreeRegressor(random_state= 0)\n","# entrenamiento\n","Tree_reg.fit(X_train_s, y_train)\n","# predicción\n","y_pred = Tree_reg.predict(X_test_s)\n","# evaluación del modelo\n","print(\"R2\",r2_score(y_test, y_pred))                             # coeficiente de determinación\n","print('MSE', mean_squared_error(y_test, y_pred))                 # error cuadrático medio\n","print('RMSE', np.sqrt(mean_squared_error(y_test,y_pred)))        # raíz error cuadrático medio"],"metadata":{"id":"H0C5vD52MVgO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <FONT SIZE=4 COLOR=\"green\"> 3.4 Modelo con Regresión Múltiple </FONT>"],"metadata":{"id":"fM8ANtWKMU3k"}},{"cell_type":"code","source":["# modelo de regresión lineal\n","modelo = LinearRegression()\n","# entrenamiento\n","modelo.fit(X_train_s,y_train)\n","# predicción\n","y_pred = modelo.predict(X_test_s)\n","# evaluación del modelo\n","print(\"R2\",r2_score(y_test, y_pred))                             # coeficiente de determinación\n","print('MSE', mean_squared_error(y_test, y_pred))                 # error cuadrático medio\n","print('RMSE', np.sqrt(mean_squared_error(y_test,y_pred)))        # raíz error cuadrático medio"],"metadata":{"id":"yjJg1suiMmqh"},"execution_count":null,"outputs":[]}]}